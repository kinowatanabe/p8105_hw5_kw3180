---
title: "p8105_hw5_kw3180"
author: "Kino Watanabe"
date: "2025-11-14"
output: github_document
---

# Load library and set theme

```{r}
library(tidyverse)
library(broom)
library(ggplot2)
library(forcats)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

# Problem 2

  * `n_subj` is # of subjects
  * `mu` is the true mean
  * `sigma` is the true sd

Generate 5000 datasets from the model x ~ Normal [ mu, sigma ] for mu = {0:6}

```{r}
sim_mean_pval = function(mu, n_subj = 30, sigma = 5) {
  
  sim_data = tibble(
    x = rnorm(n_subj, mean = mu, sd = sigma)) |> 
     summarize(
      mu_hat = mean(x),
      p_value = t.test(x, mu = 0) |> tidy() |> pull(p.value),
      reject = p_value < 0.05
    )
}

sim_results_df = 
  expand_grid(
    mu = 0:6,
    iter = 1:5000
  ) |> 
  mutate(
    estimate_df = map(mu, ~ sim_mean_pval(.x, n_subj = 30, sigma = 5))
  ) |> 
  unnest(estimate_df)
```

sim_results <- map_dfr(1:5000, ~ sim_mean_pval(n_subj  = 30, mu = 0, sigma = 5))

### Plot power vs true μ

```{r}
power_df <- sim_results_df |>
  group_by(mu) |>
  summarize(power = mean(reject), .groups = "drop")

power_df |>
  ggplot(aes(x = mu, y = power)) +
  geom_line() +
  geom_point(size = 2) +
  labs(
    title = "Power of T-Test vs True Mean (μ)",
    x = "True mean (μ)",
    y = "Test power (Proportion when rejected null)"
  ) +
    theme(legend.position = "bottom")
```

* Power is the probability of rejecting the null when the alternative hypothesis is true. As the true mean (mu) increases, the effect size increases, and the power increases (proportional association). It is easier to detect larger effect sizes, so the power is greater. When the true mean reaches around 4, the power plateaus. 


### Plot mu-hat vs mu and mu-hat in samples for which the null was rejected vs mu

  * `mu` is the true mean
  * `mu_hat` is the estimated mean
  
```{r}
estimates_df <- sim_results_df |>
  group_by(mu) |>
  summarize(
    mean_mu_hat = mean(mu_hat),               
    mean_mu_hat_reject = mean(mu_hat[reject]),
    .groups = "drop"
  )
```

```{r}
mu_plot <-
  ggplot(estimates_df, aes(x = mu)) +
    geom_line(aes(y = mean_mu_hat, color = "All simulations")) +
    geom_point(aes(y = mean_mu_hat, color = "All simulations"), size = 2) +
    geom_line(aes(y = mean_mu_hat_reject, color = "Rejected H0 only")) +
    geom_point(aes(y = mean_mu_hat_reject, color = "Rejected H0 only"), size = 2) +
    labs(
      title = "Average Estimated Mean (μ-hat) vs True μ",
      x = "True mean (μ)",
      y = "Average Estimated μ-hat",
      color = "Simulation type"
    ) +
    theme(legend.position = "bottom")

mu_plot

```

* The purple "all simulations" line is the average estimated mean across all simulations, which is closer to the true mean `mu`. This makes sense, since it takes into account of simulations that rejected and failed the rejected the null. The effect size is unbiased compared to the underlying population mean. The yellow line with the average estimated mean for rejected nulls is overestimated from the true mean, particularly when `mu` is small, as the means are among simulations with statistically significant differences. For small `mu`, significant differences between true and estimated mean can occur by chance.


# Problem 3

```{r}

raw_homicide_df =
  read_csv("data-homicides-master/homicide-data.csv",
           na = c("NA", ".","")
  )

```

* **Describe the raw data**: There are `r nrow(raw_homicide_df)` observations and `r ncol(raw_homicide_df)` variables. There are important variables about victim personal information (`victim_last`, `victim_first`, `victim_age`, `victim_sex`) and incident location/specifics (`city`, `state`, `lat`, `lon`, `reported_date`).


```{r}
homicide_df <- raw_homicide_df |>   
  janitor::clean_names() |> 
  mutate(
   state = case_match(state,
             "wI" ~ "WI",
             .default = state),
   city_state = str_c(city, ", ", state)
  ) |> 
  filter(city_state != "Tulsa, AL")

unsolved_table <- homicide_df |> 
  group_by(city_state) |> 
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% 
                               c("Closed without arrest", "Open/No arrest"), 
                             na.rm = TRUE), .groups = "drop"
  ) 

unsolved_table |> 
  knitr::kable()

```


* The most total homicides were in Chicago, IL; Philadelphia, PA; and Houston, TX. The most unsolved homicides were in  Chicago, IL; Baltimore, MD; and Houston, TX.

###### delete after finishing -- checking data
homicide_df |> 
  count(victim_age) |> 
  arrange(victim_age)
  
  
  # to describe some parts of the data
  
homicide_df |> 
  count(victim_sex) |> 
  arrange(victim_sex)
  
  
homicide_df |> 
  count(victim_sex) |> 
  arrange(victim_sex)

homicide_df |> 
  distinct(city)


homicide_df |>
  ggplot(aes(x = victim_age)) +
  geom_histogram(binwidth = 1) +
  labs(title = "Distribution of Victim Ages", x = "Age", y = "Count")

# There are 0-102 yo?

homicide_df |> 
  count(victim_race) |> 
  arrange(victim_race)


homicide_df |> 
  count(city_state) |> 
  arrange(city_state)


# no tulsa, AL??????

homicide_df |> 
  count(disposition) |> 
  arrange(disposition)


### Baltimore

```{r}
baltimore_df <- homicide_df |>
  filter(city_state == "Baltimore, MD") |>
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% 
                               c("Closed without arrest", "Open/No arrest"), na.rm = TRUE)
  ) |>
  mutate(
    prop_test = list(prop.test(unsolved_homicides, total_homicides)))
                     
baltimore_est <- baltimore_df |>
  mutate(tidy = purrr::map(prop_test, broom::tidy)) |>
  tidyr::unnest(tidy) |>
  select(estimate, conf.low, conf.high)
 
```

### All cities 

### Make a function: estimate the proportion of homicides that are unsolved with prop_test

```{r}
unsolved_prop_test <- function(unsolved_homicides, total_homicides) {
  prop.test(unsolved_homicides, total_homicides) |>
    tidy() |>
    select(estimate, conf.low, conf.high)
}

# Summarize total and unsolved homicides per city
city_summary <- homicide_df |>
  group_by(city_state) |>
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest")),
    .groups = "drop"
  )

# Run prop.test for each city using map2() in a tidy pipeline
city_results <- city_summary |>
  mutate(
    prop_test = map2(unsolved_homicides, total_homicides, unsolved_prop_test)
  ) |>
  unnest(prop_test) 

city_results |> 
  janitor::clean_names()
```

### Organize cities according to the proportion of unsolved homicides

```{r}
city_results <- city_results |>
  mutate(city_state = fct_reorder(city_state, estimate))

# Plot with error bars
ggplot(city_results, aes(x = city_state, y = estimate)) +
  geom_point(size = 2) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2, color = "darkgray") +
  coord_flip() + # Flip axes so city names are readable
  labs(
    title = "Estimated Proportion of Unsolved Homicides by City",
    x = "City, State",
    y = "Estimated Proportion of Unsolved Homicides",
    caption = "The error bars are based on the upper and lower limits"
  ) +
  theme(legend.position = "bottom")
```
