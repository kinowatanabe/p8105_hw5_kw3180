---
title: "p8105_hw5_kw3180"
author: "Kino Watanabe"
date: "2025-11-14"
output: github_document
---

# Load library and set theme

```{r}
library(tidyverse)
library(broom)
library(ggplot2)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

# Problem 2

  * `n_subj` is # of subjects
  * `mu` is the true mean
  * `sigma` is the true sd

Generate 5000 datasets from the model x ~ Normal [ mu, sigma ] for mu = {0:6}

```{r}
sim_mean_pval = function(mu, n_subj = 30, sigma = 5) {
  
  sim_data = tibble(
    x = rnorm(n_subj, mean = mu, sd = sigma)) |> 
     summarize(
      mu_hat = mean(x),
      p_value = t.test(x, mu = 0) |> tidy() |> pull(p.value),
      reject = p_value < 0.05
    )
}

sim_results_df = 
  expand_grid(
    mu = 0:6,
    iter = 1:5000
  ) |> 
  mutate(
    estimate_df = map(mu, ~ sim_mean_pval(.x, n_subj = 30, sigma = 5))
  ) |> 
  unnest(estimate_df)
```

sim_results <- map_dfr(1:5000, ~ sim_mean_pval(n_subj  = 30, mu = 0, sigma = 5))

### Plot power vs true μ

```{r}
power_df <- sim_results_df |>
  group_by(mu) |>
  summarize(power = mean(reject), .groups = "drop")

power_df |>
  ggplot(aes(x = mu, y = power)) +
  geom_line() +
  geom_point(size = 2) +
  labs(
    title = "Power of T-Test vs True Mean (μ)",
    x = "True mean (μ)",
    y = "Test power (Proportion when rejected null)"
  ) +
    theme(legend.position = "bottom")
```

* Power is the probability of rejecting the null when the alternative hypothesis is true. As the true mean (mu) increases, the effect size increases, and the power increases (proportional association). It is easier to detect larger effect sizes, so the power is greater. When the true mean reaches around 4, the power plateaus. 


### Plot mu-hat vs mu and mu-hat in samples for which the null was rejected vs mu

  * `mu` is the true mean
  * `mu_hat` is the estimated mean
  
```{r}
estimates_df <- sim_results_df |>
  group_by(mu) |>
  summarize(
    mean_mu_hat = mean(mu_hat),               
    mean_mu_hat_reject = mean(mu_hat[reject]),
    .groups = "drop"
  )
```

```{r}
mu_plot <-
  ggplot(estimates_df, aes(x = mu)) +
    geom_line(aes(y = mean_mu_hat, color = "All simulations")) +
    geom_point(aes(y = mean_mu_hat, color = "All simulations"), size = 2) +
    geom_line(aes(y = mean_mu_hat_reject, color = "Rejected H0 only")) +
    geom_point(aes(y = mean_mu_hat_reject, color = "Rejected H0 only"), size = 2) +
    labs(
      title = "Average Estimated Mean (μ-hat) vs True μ",
      x = "True mean (μ)",
      y = "Average Estimated μ-hat",
      color = "Simulation type"
    ) +
    theme(legend.position = "bottom")

mu_plot

```

* The purple "all simulations" line is the average estimated mean across all simulations, which is closer to the true mean `mu`. This makes sense, since it takes into account of simulations that rejected and failed the rejected the null. The effect size is unbiased compared to the underlying population mean. The yellow line with the average estimated mean for rejected nulls is overestimated from the true mean, particularly when `mu` is small, as the means are among simulations with statistically significant differences. For small `mu`, significant differences between true and estimated mean can occur by chance.


# Problem 3


